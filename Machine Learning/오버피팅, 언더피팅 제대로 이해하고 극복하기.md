# 1. 언더피팅

너무 적은 학습조건에 의해서 제댜로 분류되지 못하는 경우를 언더피팅이라고 합니다.

# 2. 오버피팅

너무 많은 학습조건에 의해서 제대로 분류하지 못하는 경우를 오버피팅이라고 합니다.

# 3. bias and variance

bias와 variance가 낮은 것이 오버피팅과 언더피팅을 극복할 수 있다.

http://scott.fortmann-roe.com/docs/BiasVariance.html

# 4. How to overcome underfitting?

- Find more features
- Try high variance machine learning models (Decision Tree,k-NN,SVM)

# 5. How to find if overfitting?

when test error is much higher than training error

# 6. How to overcome overfitting? (cross validation)

**K-Folds Cross Validation**

In K-Folds Cross Validation we split our data into k different subsets(or folds). We use k-1 subsets to train our data and leave the last subset (or the last fold) as test data. We then average the model against each of the folds and then finalize our model. After that we test it against the tset set.

1. During training, test against validation data
2. if validation accuracy is lower, do regularization!
3. repeat regularization until no overfitting

오버피팅 극복하기

1. regularization
2. early stopping - deep learning
3. drop out - deep learning